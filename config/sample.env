# Mistral AI Inference Server Configuration
# Rename this file to .env and update values as needed

# Model configuration
MODEL_ID=mistralai/Mistral-7B-Instruct-v0.1
TENSOR_PARALLEL_SIZE=2
GPU_MEMORY_UTILIZATION=0.90
MAX_MODEL_LEN=8192
DTYPE=bfloat16
TRUST_REMOTE_CODE=true
ENFORCE_EAGER=false
MAX_PARALLEL_REQUESTS=128

# Server configuration
PORT=8000
HOST=0.0.0.0

# Hugging Face credentials (if needed for private models)
HUGGING_FACE_HUB_TOKEN=your_hf_token_here

# Monitoring configuration (SigNoz)
ENABLE_MONITORING=true
OTLP_ENDPOINT=http://signoz-otel-collector:4317
SERVICE_NAME=mistral-inference-server

# SigNoz - add this if you're deploying SigNoz separately
# SIGNOZ_ACCESS_TOKEN=your_signoz_token_here
