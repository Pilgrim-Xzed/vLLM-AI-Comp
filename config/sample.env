# Llama 8B Inference Server Configuration
# Rename this file to .env and update values as needed

# Model configuration
MODEL_ID=meta-llama/Meta-Llama-3-8B
TENSOR_PARALLEL_SIZE=2
GPU_MEMORY_UTILIZATION=0.95
MAX_MODEL_LEN=8192
QUANTIZATION=awq
DTYPE=half
TRUST_REMOTE_CODE=true
ENFORCE_EAGER=false
MAX_PARALLEL_REQUESTS=512
DISABLE_TRACES=true
BLOCK_SIZE=16
SWAP_SPACE=4

# Server configuration
PORT=8000
HOST=0.0.0.0
API_WORKERS=1
TIMEOUT_KEEP_ALIVE=120
BACKLOG=2048

# Hugging Face credentials (if needed for private models)
HUGGING_FACE_HUB_TOKEN=your_hf_token_here

# Monitoring configuration (SigNoz)
ENABLE_MONITORING=true
OTLP_ENDPOINT=http://signoz-otel-collector:4317
SERVICE_NAME=llama-inference-server
