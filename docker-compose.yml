version: '3.8'

services:
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: mistral-inference-server
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "8000:8000"
    volumes:
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface
    # Configuration for dual H100 GPUs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2  # Using 2 H100 GPUs
              capabilities: [gpu, utility, compute]
    environment:
      - MODEL_ID=${MODEL_ID:-mistralai/Mistral-7B-Instruct-v0.1}
      - TENSOR_PARALLEL_SIZE=2
      - GPU_MEMORY_UTILIZATION=0.90
      - MAX_MODEL_LEN=8192
      - DTYPE=bfloat16
      - TRUST_REMOTE_CODE=true
      - ENFORCE_EAGER=false
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - ENABLE_MONITORING=true
      - OTLP_ENDPOINT=${OTLP_ENDPOINT:-http://signoz-otel-collector:4317}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Increased startup time for model loading

  # SigNoz deployment for monitoring
  # This assumes SigNoz is deployed separately in production
  # For local development, uncomment this section
  #
  # signoz-otel-collector:
  #   image: signoz/signoz-otel-collector:latest
  #   container_name: signoz-otel-collector
  #   restart: unless-stopped
  #   command: ["--config=/etc/otel-collector-config.yaml"]
  #   volumes:
  #     - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml
  #   ports:
  #     - "4317:4317"  # OTLP gRPC receiver
  #     - "4318:4318"  # OTLP HTTP receiver
  #   environment:
  #     - SIGNOZ_ACCESS_TOKEN=${SIGNOZ_ACCESS_TOKEN}

volumes:
  hf_cache:
